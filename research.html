            <!DOCTYPE html>
                        <html lang="en-US"
                            class="html_stretched responsive av-preloader-disabled  html_header_top html_logo_left html_main_nav_header html_menu_right html_slim html_header_sticky html_header_shrinking_disabled html_header_transparency html_mobile_menu_tablet html_header_searchicon html_content_align_center html_header_unstick_top_disabled html_header_stretch_disabled html_av-overlay-side html_av-overlay-side-classic html_av-submenu-noclone html_entry_id_2 av-cookies-no-cookie-consent av-default-lightbox av-no-preview html_text_menu_active av-mobile-menu-switch-default">

                        <head>
                            <meta charset="UTF-8" />
                            <!-- mobile setting -->
                            <meta name="viewport" content="width=device-width, initial-scale=1">
                            <!-- Scripts/CSS and wp_head hook -->
                            <meta name='robots' content='index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1' />

                            <title>Home - Chirag Agarwal</title>
                            <link rel="canonical" href="https://chirag-agarwall.github.io/" />
                            <meta property="og:locale" content="en_US" />
                            <meta property="og:type" content="website" />
                            <meta property="og:title" content="Home - Chirag Agarwal" />
                            <meta property="og:url" content="https://chirag-agarwall.github.io/" />
                            <meta property="og:site_name" content="Chirag Agarwal" />
                            <!-- <meta property="article:modified_time" content="2021-10-22T02:34:37+00:00" /> -->
                            <meta property="og:image" content="images/background.jpg" />

                            <link rel='stylesheet' id='avia-grid-css' href='css/style.css' type='text/css' media='all' />
                            <link rel='stylesheet' id='avia-base-css' href='css/base.css' type='text/css' media='all' />
                            <link rel='stylesheet' id='avia-layout-css' href='css/layout.css' type='text/css' media='all' />
                            <link rel='stylesheet' id='avia-layout-css' href='css/grid.css' type='text/css' media='all' />

                            <link rel='stylesheet' id='avia-dynamic-css' href='css/avia-dynamic.css' type='text/css' media='all' />
                            <script type='text/javascript' src='https://ajax.googleapis.com/ajax/libs/jquery/3.6.1/jquery.min.js'
                                id='jquery-core-js'></script>
                            <link href="https://fonts.googleapis.com/css?family=Poppins:300,400,500,600,700" rel="stylesheet">
                            <link href="https://fonts.googleapis.com/css?family=Montserrat:300,400,500,700" rel="stylesheet">
                            <link rel="stylesheet" href="css/open-iconic-bootstrap.min.css">
                            <link rel="stylesheet" href="css/animate.css">
                            <link rel="stylesheet" href="css/owl.carousel.min.css">
                            <link rel="stylesheet" href="css/owl.theme.default.min.css">
                            <link rel="stylesheet" href="css/magnific-popup.css">
                            <link rel="stylesheet" href="css/aos.css">
                            <link rel="stylesheet" href="css/ionicons.min.css">
                            <link rel="stylesheet" href="css/bootstrap-datepicker.css">
                            <link rel="stylesheet" href="css/jquery.timepicker.css">
                            <link rel="stylesheet" href="css/flaticon.css">
                            <link rel="stylesheet" href="css/icomoon.css">
                            <link rel="stylesheet" href="css/style.css">
                        </head>

                        <body id="top" itemscope="itemscope" itemtype="https://schema.org/WebPage">
                            <div id="research">
                            <div id='wrap_all' class="responsive_wrap">
                 <header id='header'
                        class='all_colors header_color light_bg_color  av_header_top av_logo_left av_main_nav_header av_menu_right av_slim av_header_sticky av_header_shrinking_disabled av_header_stretch_disabled av_mobile_menu_tablet av_header_transparency av_header_searchicon av_header_unstick_top_disabled av_bottom_nav_disabled  av_header_border_disabled' 
                        role="banner" itemscope="itemscope" itemtype="https://schema.org/WPHeader">
                        <div id='header_main' class='container_wrap container_wrap_logo'>
                            <div class='container av-logo-container'>
                                <div class='inner-container' style="display:flex; justify-content: space-around;">

                                    <div class='logo avia-standard-logo header-name' style="width:1000px">
                                        <div class="header_title header_flex "><a href='https://chirag-agarwall.github.io/'>AIKYAM LAB</a></div>
                                         <div class="header_flex" > 
                                          <nav class='main_menu navigation-desktop' data-selectname='Select a page' role="navigation" itemscope="itemscope"
                                        itemtype="https://schema.org/SiteNavigationElement" >
                                        <div class="avia-menu av-main-nav-wrap av_menu_icon_beside">
                                            <ul id="avia-menu" class="menu av-main-nav" >
                                        
                                                <li id="menu-item-785"
                                                    class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-top-level menu-item-top-level-1">
                                                    <a href="https://chirag-agarwall.github.io/" itemprop="url"><span
                                                            class="avia-bullet"></span><span class="avia-menu-text">Home</span><span
                                                            class="avia-menu-fx"><span class="avia-arrow-wrap"><span
                                                                    class="avia-arrow"></span></span></span></a>
                                                </li>
                                                <li id="menu-item-787"
                                                    class="menu-item menu-item-type-post_type menu-item-object-page  menu-item-top-level menu-item-top-level-3">
                                                    <a href="./about.html"><span
                                                            class="avia-bullet"></span><span
                                                            class="avia-menu-text">About</span><span
                                                            class="avia-menu-fx"><span class="avia-arrow-wrap"><span
                                                                    class="avia-arrow"></span></span></span></a>
                                                </li>
                                                <li id="menu-item-787"
                                                    class="menu-item menu-item-type-post_type menu-item-object-page current-menu-item page_item page-item-2 current_page_item menu-item-top-level menu-item-top-level-3">
                                                    <a href="research.html"><span
                                                            class="avia-bullet"></span><span
                                                            class="avia-menu-text">Research</span><span
                                                            class="avia-menu-fx"><span class="avia-arrow-wrap"><span
                                                                    class="avia-arrow"></span></span></span></a>
                                                </li>
                                                <li id="menu-item-787"
                                                    class="menu-item menu-item-type-post_type menu-item-object-page menu-item-top-level menu-item-top-level-3">
                                                    <a href="publications.html"><span
                                                            class="avia-bullet"></span><span
                                                            class="avia-menu-text">Publications</span><span
                                                            class="avia-menu-fx"><span class="avia-arrow-wrap"><span
                                                                    class="avia-arrow"></span></span></span></a>
                                                </li>
                                                <li id="menu-item-787"
                                                    class="menu-item menu-item-type-post_type menu-item-object-page  menu-item-top-level menu-item-top-level-3">
                                                    <a href="team.html"><span
                                                            class="avia-bullet"></span><span
                                                            class="avia-menu-text">Team</span><span
                                                            class="avia-menu-fx"><span class="avia-arrow-wrap"><span
                                                                    class="avia-arrow"></span></span></span></a>
                                                </li>
                                                <li id="menu-item-835"
                                                    class="menu-item menu-item-type-post_type menu-item-object-page  menu-item-top-level menu-item-top-level-4">
                                                    <a href="teaching.html"><span
                                                            class="avia-bullet"></span><span
                                                            class="avia-menu-text">Teaching</span><span class="avia-menu-fx"><span
                                                                class="avia-arrow-wrap"><span
                                                                    class="avia-arrow"></span></span></span></a>
                                                </li>
                                                <li id="menu-item-835"
                                                    class="menu-item menu-item-type-post_type menu-item-object-page  menu-item-top-level menu-item-top-level-4">
                                                    <a href="outreach.html"><span
                                                            class="avia-bullet"></span><span
                                                            class="avia-menu-text">Outreach</span><span class="avia-menu-fx"><span
                                                                class="avia-arrow-wrap"><span
                                                                    class="avia-arrow"></span></span></span></a>
                                                </li>
                                                <li id="menu-item-835"
                                                    class="menu-item menu-item-type-post_type menu-item-object-page  menu-item-top-level menu-item-top-level-4">
                                                    <a href="talks.html"><span
                                                            class="avia-bullet"></span><span
                                                            class="avia-menu-text">Talks</span><span class="avia-menu-fx"><span
                                                                class="avia-arrow-wrap"><span
                                                                    class="avia-arrow"></span></span></span></a>
                                                </li>
                                                <li class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home page_item page-item-2 menu-item-top-level menu-item-top-level-1 open_position_responsive">
                                                    <a href="./positions.html"> Open Positions </a>
                                                </li>
                                       
                                                <li class="av-burger-menu-main menu-item-avia-special ">
                                                    <a href="#" aria-label="Menu" aria-hidden="false">
                                                        <span class="av-hamburger av-hamburger--spin av-js-hamburger">
                                                            <span class="av-hamburger-box">
                                                                <span class="av-hamburger-inner"></span>
                                                                <strong>Menu</strong>
                                                            </span>
                                                        </span>
                                                        <span class="avia_hidden_link_text">Menu</span>
                                                    </a>
                                                </li>


                                            </ul>
                                        </div>
                                        
                                    </nav>
                                </div>
                                        </div>
                                  
                                    <div class="header_flex open_position"><a href="./positions.html" style="color:black; text-decoration: none"> Open Positions </a></div>
                                </div>
                            </div>
                            <!-- end container_wrap-->
                        </div>
                        <div class='header_bg'></div>

                        <!-- end header -->
                    </header>

                            <div id="main" class="all_colors main-publ" data-scroll-offset="88">
                                <div class="parallax page_header_background">
                                    <!--layer 1 will be the bottom layer, layer 3 will be the top layer-->
                                    <div class="parallax-container layer1 page_header_position" style="color:    white; padding-top:140px; justify-content: flex-start;">
                                        <div class='avia_textblock  ' style='font-size:35px; ' itemprop="text">
                                            <p class="image-text">
                                                    RESEARCH
                                            </p>
                                        </div>
                                    </div>
                                </div>
                         
                                <div class="main_color container_wrap_first container_wrap sidebar_right">
                                    <div class="container av-section-cont-open">
                                        <main role="main" itemprop="mainContentOfPage" class="template-page av-content alpha units">
                                            <div class='post-entry post-entry-type-page post-entry-2'>
                                                <div class='entry-content-wrapper clearfix'>
                                                    <section class="section research_section">
                                                        <article>
<!--                                                             <div align="center">
                                                                <img class="research-main-img" style="max-width: 63%; max-height: 45%;" src="./images/research_overview.png">
                                                                <h5 style="display: flex;justify-content:center;">An overview of the pipeline for developing and benchmarking Trustworthy models at different stages of the machine learning model pipeline.</h5>
                                                            </div> -->
                                                            <!-- <br> -->
                                                            <!-- <br> -->
                                                       <div class="research_content">
                                                            <h2 id="" style="text-transform: capitalize; font-size: 2.2rem;">
                                                                Explainable Artificial Intelligence
                                                            </h2>
                                                            <p style="text-align: justify;">
                                                                Explainable AI (XAI) is pivotal for fostering trust in AI systems by making their decision-making processes transparent and interpretable to humans. Our research in XAI focuses on overcoming the inherent complexity of modern AI models, particularly frontier models and black-box systems, to produce reliable and actionable explanations. Our work addresses the trade-off between model performance and explainability, ensuring that explanations are not only understandable but also faithful to the model's actual behavior, which is critical for applications where stakeholders—such as doctors, judges, or policymakers—require clear justifications for AI-driven decisions.
                                                            </p>
                                                            <p style="text-align: justify;">
                                                                Some recent key contributions of our work is the reliability of explanations from LLMs. Our work investigates why LLM-generated explanations often appear plausible to humans but may fail to accurately reflect the model's decision-making process. By analyzing the trade-offs between faithfulness (how well an explanation captures the model's reasoning) and plausibility (how convincing it seems), our work proposes strategies to improve explanation quality, such as refining prompting techniques and developing evaluation metrics. This research is foundational for ensuring that LLMs, increasingly used in critical applications, provide trustworthy insights.
                                                                
                                                                In addition, we also tackle the challenge of eliciting 
                                                                <spam class="tooltip-container">
                                                                    faithful Chain-of-Thought (CoT) 
                                                                    <spam class="tooltip-image">
                                                                        <a href="https://arxiv.org/pdf/2402.04614" target="_blank">
                                                                          <img src="./images/faithful_cot.jpg" alt="Tooltip Image">
                                                                        </a> 
                                                                    </spam>
                                                                </spam> reasoning in LLMs. Our studies have been cited in <a href="https://cdn.openai.com/o1-system-card-20241205.pdf" target="_blank">OpenAI o1 System Card</a> and reveal the limitations of current approaches like in-context learning, LoRA fine-tuning, and activation editing in guaranteeing accurate reasoning paths, advocating for new architectural designs and training paradigms to enhance transparency in LLMs, paving the way for more reliable decision-making in domains like legal and medical AI.
                                                            </p>
                                                            <p style="text-align: justify;">
                                                                In our work, we develop new explainability algorithms to study the behavior of complex black-box unimodal and multimodal models. The research on multimodal explanation methods is at a very nascent stage and we aim to introduce novel algorithms that generate actionable explanations for multimodal models. Through these efforts, our lab aims to shape the future of XAI, ensuring AI systems are both powerful and interpretable, with far-reaching implications for ethical AI deployment.
                                                            </p>
                                                            <!-- <p>
                                                                A popular strategy for explaining the decision of a predictive and generative model is attributing their decision to either the i) input features (i.e., feature attribution) and/or ii) samples from the training datasets (i.e., data attribution). Intuitively, an attribution map on the input represents a heatmap that highlights input features (pixles in images, tokens in language, nodes/edges in graphs, temporal window for time series and trajectories in RL agents) that are the evidence for and against the model outputs. A widely used technique to construct attribution maps is to approximate the attribution value of an input region by the probability change when that region is absent, i.e., removed from the input.
                                                            </p> -->
<!--                                                             <div align="center">
                                                                <img src="./images/xai-algo.png">
                                                            </div>   -->                                        
                                                            <!-- <p>
                                                                While removing an input feature to measure its attribution is a principle method
                                                                (i.e., “intervention” in causal reasoning), a key open question is: <em style="color:#ffffcc">How to remove?</em> The removal of input features often lead to out-of-distribution (OoD) inputs (e.g., noisy images, perturbed tokens, disconnceted graphs, etc.) on which the underlying models were not trained. Because ML models are often easily fooled by unusual input patterns, we hypothesize that such examples might yield explanations that are unfaithful, i.e., explanations that do not represent the internal reasoning of the model. We develop several attribution-based techniques that generate explanations that are faithful to the underlying model and interpretable to human users and stakeholders. For example, we introduced generative-based attribution strategies that can be augmented with existing feature attribution algorithms to generate more faithful and accurate explanation and extend these techniques to RL agents, where we attribute the policy decisions of a trained RL agent to the trajectories encountered by it during training.
                                                            </p> -->
                                                            <h4 id="example-related-publications">Relevant Papers:</h4>
                                                            <p>
                                                                <a href="https://arxiv.org/pdf/2502.09457" target="_blank">The Multilingual Mind: A Survey of Multilingual Reasoning in Language Models</a><br>
                                                                <a href="https://arxiv.org/pdf/2411.15382" target="_blank">On the Impact of Fine-Tuning on Chain-of-Thought Reasoning</a><br>
                                                                <a href="https://proceedings.mlr.press/v238/harsha-tanneru24a/harsha-tanneru24a.pdf" target="_blank">Quantifying Uncertainty in Natural Language Explanations of Large Language Models</a><br>
                                                                <a href="https://arxiv.org/pdf/2501.05078" target="_blank">Analyzing Memorization in Large Language Models through the Lens of Model Attribution</a><br>
                                                                <a href="https://arxiv.org/pdf/2402.04614" target="_blank">Faithfulness vs. Plausibility: On the (un)Reliability of Explanations from Large Language Models</a><br>
                                                                <a href="https://openreview.net/pdf?id=3h0kZdPhAC" target="_blank">On the Difficulty of Faithful Chain-of-Thought Reasoning in Large Language Models</a><br>
                                                                <a href="https://arxiv.org/pdf/2310.05797" target="_blank">In-context Explainers: Harnessing LLMs for Explaining Black-Box Models</a><br>
                                                                <a href="" target="_blank">Openxai: Towards a transparent evaluation of model explanations</a><br>
                                                                <a href="https://www.nature.com/articles/s41597-023-01974-x" target="_blank">Evaluating Explainability for Graph Neural Networks</a><br>
                                                                <!-- <a href="https://arxiv.org/pdf/2310.05797" target="_blank">Are Large Language Models Post Hoc Explainers?</a><br>
                                                                <a href="https://openaccess.thecvf.com/content/ACCV2020/papers/Agarwal_Explaining_image_classifiers_by_removing_input_features_using_generative_models_ACCV_2020_paper.pdf" target="_blank">Explaining Image Classifiers by Removing Input Features Using Generative Models</a><br>
                                                                <a href="https://arxiv.org/pdf/2305.04073" target="_blank">Explaining RL Decisions with Trajectories</a><br>
                                                                <a href="https://openreview.net/pdf?id=m3NuXLzMoP" target="_blank">Intriguing Properties of Visual-Language Model Explanations</a><br>
                                                                <a href="https://openreview.net/pdf?id=m3NuXLzMoP" target="_blank">Towards the Unification and Robustness of Perturbation- and Gradient-based Explanations</a><br> -->
                                                            </p>
                                                            <!-- <p class="read-more"><a href="#" class="btn btn-danger">
                                                                    <svg class="arrows">
                                                              <path class="a2" d="M0 20 L20 42 L40 20"></path>
                                                              <path class="a3" d="M0 40 L20 62 L40 40"></path>
                                                            </svg>
                                                                Read More</a></p> -->
                                                            </div>
                                                    <div class="line-break"><hr></div>
                                                    <div class="research_content">
                                                        <h2 id="large-scale-xai-benchmarking" style="text-transform: capitalize; font-size: 2.2rem;">
                                                            AI Safety and Alignment
                                                        </h2>
                                                        <p style="text-align:justify;">
                                                            AI Safety and Alignment is a cornerstone of current AI research that focuses on ensuring that AI systems are robust, reliable, and free from unintended consequences, particularly in high-stakes environments. Our work addresses critical risks, such as adversarial attacks, model hallucinations, and privacy violations, which can undermine the trustworthiness of AI systems. By developing rigorous evaluation benchmarks and certification methods, our research aims to safeguard AI deployment in domains like healthcare, where errors can have life-altering consequences. Our contributions are particularly timely as AI systems, especially large language and vision-language models, become integral to real-world applications.
                                                        </p>
                                                        <p style="text-align:justify;">
                                                            Our recent AI Safety work focused on evaluating and mitigating <a>hallucinations in large-vision language models (LVLMs)</a>, where we show that state-of-the-art LVLMs generate false or misleading outputs and pose a major safety risk. In addition to hallucination benchmark, we have also introduced comprehensive benchmark for assessing LLM safety in healthcare settings, where we evaluate models on their ability to provide accurate and safe medical advice, addressing risks like misdiagnosis or harmful recommendations. By integrating real-world medical scenarios and stress-testing models under adversarial conditions, our benchmark sets a new standard for safety evaluation. Further, we have developed methods to certify that LLMs can withstand malicious prompts designed to elicit harmful or incorrect responses. By combining theoretical analysis with empirical testing, our work ensures that models remain safe under real-world threats. Additionally, our exploration of iterative prompting’s impact on truthfulness in LLMs, fairness in LVLMs and study on operationalizing data protection rights in LLMs underscores our holistic approach to AI Safety, addressing both technical and ethical dimensions and advancing the field, ensuring AI systems are secure and aligned with societal needs.
                                                        </p>
                                                        <p>
                                                            Recent advancements in LLMs have introduced new security risks, particularly in the form of jailbreak attacks that bypass safety mechanisms. While previous work has identified both semantic and non-semantic tokens capable of jailbreaking frontier models using pre-defined responses like <em> “Sure, ...” </em>, these studies have largely overlooked the potential for models to create an internal, model-specific “language” of non-semantic text that reliably triggers specific behaviors. For example, a non-semantic phrase that appears as a simple typo or misspelling to a naive user could consistently prompt the model to produce malicious code, posing a serious threat. Despite prior work on prompt engineering and adversarial attacks, a systematic, mechanistic understanding of how these vulnerabilities arise—and how to mitigate them—remains an open challenge, which we aim to explore in our future work.
                                                        </p>
                                                        <!-- <div align="center">
                                                            <img src="./images/xai_research.png">
                                                        </div> -->
                                                        <h4 id="example-related-publications">Relevant Papers:</h4>
                                                        <p>
                                                            <a href="https://arxiv.org/pdf/2412.20622" target="_blank">Towards a Systematic Evaluation of Hallucinations in Large-Vision Language Models</a><br>
                                                            <a href="https://arxiv.org/abs/2411.08506" target="_blank">Towards Operationalizing Right to Data Protection</a><br>
                                                            <a href="https://arxiv.org/pdf/2402.06625" target="_blank">Understanding the effects of iterative prompting on truthfulness</a><br>
                                                            <a href="https://openreview.net/pdf?id=cFyagd2Yh4" target="_blank"> MedSafetyBench: Evaluating and Improving the Medical Safety of Large Language Models</a><br>
                                                            <a href="https://arxiv.org/pdf/2309.02705" target="_blank">Certifying LLM Safety against Adversarial Prompting</a><br>
                                                            <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Seth_DeAR_Debiasing_Vision-Language_Models_With_Additive_Residuals_CVPR_2023_paper.pdf" target="_blank">Dear: Debiasing vision-language models with additive residuals</a><br>
                                                            <a href="https://arxiv.org/abs/2302.13406" target="_blank">GNNDelete: A general unlearning strategy for graph neural networks</a><br>
                                                        </p>
                                                        <!-- <p class="read-more"><a href="#" class="btn btn-danger">
                                                            <svg class="arrows">
                                                                <path class="a2" d="M0 20 L20 42 L40 20"></path>
                                                                <path class="a3" d="M0 40 L20 62 L40 40"></path>
                                                            </svg>
                                                            Read More</a>
                                                        </p> -->
                                                    </div>                                                                     
                                                    <!-- <div class="research_content"> -->
                                                        <!-- <h2 id="large-scale-xai-benchmarking" style="text-transform: capitalize; font-size: 2.2rem;">
                                                            Large Scale Benchmarking of Post Hoc Explainers for Diverse Data Modalities
                                                        </h2> -->
                                                        <!-- <p>
                                                            The increasing use of predictive and generative models in high-stake domains including healthcare, law, and finance requires us to develop algorithms and techniques that can explain the behavior fo these complex machine learning models. There has been a growing demand on explaining the behavior of these large-scale models so that both model developers and stakeholders can understand the rationale behind their predictions or responses, and determine if and when to rely on them. To this end, various techniques have been proposed in recent literature to generate post hoc explanations of individual predictions made by complex ML models trained on different data modalities like image, text, graphs, and tables. In general, these explanation methods output the influence of each of the input features on the model's output and is observed as a proxy for model explanation. However, it is critical to answer -- <em style="color:#ffffcc">which explanation methods are effective with respect to what notions of reliability and under what conditions?</em> Answering this question will ensure that the explanations generated by these methods are reliable so that relevant stakeholders and decision makers are provided with credible information about the underlying models.
                                                        </p> -->
                                                        <!-- <div align="center">
                                                            <img src="./images/xai_research.png">
                                                        </div> -->
                                                        <!-- <p>
                                                            We take the first step towards answering this question by designing benchmarks with a broader ecosystem of data loaders, data processing functions, explanation methods, evaluation metrics (e.g., accuracy, faithfulness, stability, fairness), and explanation visualizers, to reliably benchmark the quality of any given explanation for models trained on different modalities. We provide open-source implementations and ready-to-use API interfaces for different post hoc explanation methods, and release a collection of predictive models trained on synthetic and real-world datasets for benchmarking explanation methods to promote transparency, and to allow users to easily compare the performance of multiple explanation methods across a wide variety of synthetic and real-world datasets, evaluation metrics, and predictive models.
                                                        </p> -->

                                                        <!-- <p>
                                                            We are looking to develop easy-to-use reproducible benchmarks (synthetic datasets and evaluation metrics) to quantify explanations generated for multimodal models like LVLMS, where explanations are not limited to one data modality.
                                                        </p> -->

                                                        <!-- <h4 id="example-related-publications">Relevant Papers:</h4> -->

                                                        <!-- <p>
                                                            <a href="https://proceedings.mlr.press/v238/harsha-tanneru24a/harsha-tanneru24a.pdf" target="_blank">Quantifying Uncertainty in Natural Language Explanations of LLMs</a><br>
                                                            <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/65398a0eba88c9b4a1c38ae405b125ef-Paper-Datasets_and_Benchmarks.pdf" target="_blank">OpenXAI: Towards a Transparent Evaluation of Post hoc Model Explanations</a><br>
                                                            <a href="https://proceedings.mlr.press/v151/agarwal22b/agarwal22b.pdf" target="_blank">Probing GNN Explainers: A Rigorous Theoretical and Empirical Analysis of GNN Explanation Methods</a><br>
                                                            <a href="https://www.nature.com/articles/s41597-023-01974-x" target="_blank">Evaluating Explainability for Graph Neural Networks</a><br>
                                                            <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Bansal_SAM_The_Sensitivity_of_Attribution_Methods_to_Hyperparameters_CVPR_2020_paper.pdf" target="_blank">SAM: The Sensitivity of Attribution Methods to Hyperparameters</a><br>
                                                        </p> -->
                                                        <!-- <p class="read-more"><a href="#" class="btn btn-danger">
                                                            <svg class="arrows">
                                                                <path class="a2" d="M0 20 L20 42 L40 20"></path>
                                                                <path class="a3" d="M0 40 L20 62 L40 40"></path>
                                                            </svg>
                                                            Read More</a>
                                                        </p> -->
                                                    <!-- </div> -->
                                                    <!-- <div class="line-break"><hr></div> -->
                                                    <!-- <div class="research_content"> -->
<!--                                                         <h2 style="text-transform: capitalize; font-size: 2.2rem;">
                                                            Understanding Training Dynamics of Large Scale Models and Making them More Trustworthy during Inference
                                                        </h2> -->
                                                        <!-- <p>
                                                            A myriad of machine learning models have been trained under supervised, unsupervised, or self-supervised settings using different predictive and trustworthy properties. While these models do achieve state-of-the-art performance on downstream predictive and generative tasks, we have little understanding of their exact training dynamics, e.g., which examples are more important for the model to converge? which examples do the model learn first? does harder examples ends up getting memorized? the contrast between early- and late-training stages, etc. In addition, previous researches have shown that it is possible to train deep neural networks with fairness, robustness, and explainability properties, but haven't yet delivered a computationally efficient way to train large-scale visual-language and large language models. Further, <em style="color:#ffffcc">different pillars of TrustworthyML do promise great success independently, but they are primarily discussed in dedicated silos,</em> and it remains unclear whether a relation exists between them. Hence, we focus on three key questions with respect to training large-scale trustworthy models:
                                                            <div align="center">
                                                                <img src="./images/training.png">
                                                            </div>
                                                            <ol>
                                                                <li>
                                                                    When and which model parameters i) learn to memorize, ii) start hallucinating, iii) are prone to noise, and iv) learn biased features during the training process?
                                                                </li>
                                                                <li>
                                                                    How to incorporate and enhance the trustworthy properties at different stages (early or late) of the training pipeline?
                                                                </li>
                                                                <li>
                                                                    What is the inter-connection and trade-off between different trustworthy properties? 
                                                                </li>
                                                            </ol>
                                                        </p> --> 
                                                        <!-- <p>
                                                            Addressing these questions will help us understand the training dynamics of a model and, moving forward, we aim to develop architectures and training algorithms that will lead to more trustworthy models. For instance, we have developed techniques to understand the training dynamics of a ML model, where we use gradient explanations to identify samples that are easier/harder for the model to learn (see <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Agarwal_Estimating_Example_Difficulty_Using_Variance_of_Gradients_CVPR_2022_paper.pdf" target="_blank">VoG</a>). 
                                                        </p> -->
                                                        <!-- The other aspect of model training that we focus on is to understand the training dynamics of a model or developing architectures that leverage explanations in their training pipeline. We have developed techniques to understand the training dynamics of a ML model, where we identify samples that are easier/harder for the model to learn. Moving forward, we intend to focus on leveraging explanations in identifying a subset of relatively more challenging examples during model training of large foundation models to help prioritize limited human annotation and auditing time. Further, interpretable frameworks can be immensely useful in safety-critical applications as they allow information/features to flow along important neural paths while blocking information along irrelevant paths to improve the accuracy and explainability of models. -->
                                                        <!-- <div align="center" class="flex-diff">
                                                            <img src="./images/inference.png">
                                                            <img src="./images/rl_algo.gif">
                                                        </div> -->
                                                        <!-- <p>
                                                            Further, recent years have seen an enormous growth in the use of large-scale models that are trained on broad uncurated datasets using self-supervised learning and adapted for a wide range of downstream tasks. In contrast to supervised models, self-supervised pretrained models learn and embed general patterns and features from the data in a densely packed representation space and in most cases we are only provided with the output representation of these models, and we have little to no understanding of how to interpret them. Once we unfold this representation space, it is very hard to understand what these closed models learn. The way we interpret this is that <em style="color:#ffffcc">large models learn data features and patterns and embed them in a densely packed representation space</em>, where we have small feature pockets that can be used for specific downstream tasks and tools like in-context learning (ICL) and chain-of-thoughts (CoT) helps us to navigate this complex representation space. The question then becomes -- <em style="color:#ffffcc">how unique and trustworthy are these representations?</em>
                                                        </p> -->
                                                        <!-- <p>
                                                            Most organizations training visual-language and large language models provide their representations as an Encoder service, where users can employ the representations from these models to their respective downstream tasks. Hence, it's important to disentangle the representations from these models and understand their trustworthy properties. We explore different robustness, explainability, uncertainty, and fairness properties using inference- and API-level access of visual-language and large language models.
                                                        </p> -->
                                                        <!-- <h4 id="example-related-publications">Relevant Papers:</h4> -->
                                                        <!-- <p>
                                                            <a href="https://arxiv.org/pdf/2107.13098" target="_blank">A Tale of Two Long Tails</a><br>
                                                            <a href="https://proceedings.mlr.press/v161/agarwal21b/agarwal21b.pdf" target="_blank">Towards a Unified Framework for Fair and Stable Graph Representation Learning</a><br>
                                                            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Agarwal_Estimating_Example_Difficulty_Using_Variance_of_Gradients_CVPR_2022_paper.pdf" target="_blank">Estimating Example Difficulty using Variance of Gradients</a><br>
                                                            <a href="https://proceedings.mlr.press/v198/giunchiglia22a/giunchiglia22a.pdf" target="_blank">Towards Training GNNs using Explanation Directed Message Passing</a><br>
                                                            <a href="https://openreview.net/pdf?id=sXYJpfoW1V" target="_blank">Towards Fair Knowledge Distillation using Student Feedback</a><br>
                                                            <a href="https://arxiv.org/pdf/2403.03744" target="_blank"> Towards Safe Large Language Models for Medicine</a><br>
                                                            <a href="https://arxiv.org/pdf/2309.16452" target="_blank">On the Trade-offs between Adversarial Robustness and Actionable Explanations</a><br>
                                                            <a href="https://proceedings.mlr.press/v139/agarwal21c/agarwal21c.pdf" target="_blank">Towards the Unification and Robustness of Perturbation and Gradient Based Explanations</a><br>
                                                            <a href="https://proceedings.mlr.press/v151/pawelczyk22a/pawelczyk22a.pdf" target="_blank">Exploring Counterfactual Explanations Through the Lens of Adversarial Examples: A Theoretical and Empirical Analysis</a><br>
                                                            <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Seth_DeAR_Debiasing_Vision-Language_Models_With_Additive_Residuals_CVPR_2023_paper.pdf" target="_blank">DeAR: Debiasing Vision-Language Models with Additive Residuals</a><br>
                                                            <a href="https://arxiv.org/pdf/2307.13192" target="_blank">Counterfactual Explanation Policies in RL</a><br>
                                                            <a href="https://arxiv.org/pdf/2309.02705" target="_blank">Certifying LLM Safety against Adversarial Prompting</a>
                                                        </p> -->
                                                        <!-- <p class="read-more"><a href="#" class="btn btn-danger">
                                                            <svg class="arrows">
                                                      <path class="a2" d="M0 20 L20 42 L40 20"></path>
                                                      <path class="a3" d="M0 40 L20 62 L40 40"></path>
                                                    </svg>
                                                        <span>Read More</span></a></p> -->
                                                    <!-- </div> -->
                                                    <!-- <div class="line-break"><hr></div> -->
<!--                                                     <div class="research_content">
                                                        <h2 id="learning-trustworthy-representations" style="text-transform: capitalize; font-size: 2.2rem;">
                                                                Making Pre-Trained Models More Trustworthy during Inference
                                                        </h2>
                                                        <p>
                                                            Recent years have seen an enormous growth in the use of large-scale models that are trained on broad uncurated datasets using self-supervised learning and adapted for a wide range of downstream tasks. In contrast to supervised models, self-supervised pretrained models learn and embed general patterns and features from the data in a densely packed representation space and in most cases we are only provided with the output representation of these models, and we have little to no understanding of how to interpret them. Once we unfold this representation space, it is very hard to understand what these closed models learn. The way we interpret this is that <em style="color:#ffffcc">large models learn data features and patterns and embed them in a densely packed representation space</em>, where we have small feature pockets that can be used for specific downstream tasks and tools like in-context learning (ICL) and chain-of-thoughts (CoT) helps us to navigate this complex representation space. The question then becomes -- <em style="color:#ffffcc">how unique and trustworthy are these representations?</em>
                                                        </p>
                                                        <div align="center">
                                                            <img src="./images/inference.png">
                                                            <img src="./images/rl_algo.gif">
                                                        </div>
                                                        <p>
                                                            Most organizations training visual-language and large language models provide their representations as an Encoder service, where users can employ the representations from these models to their respective downstream tasks. Hence, it's important to disentangle the representations from these models and understand their trustworthy properties. We explore different robustness, explainability, uncertainty, and fairness properties using inference- and API-level access of visual-language and large language models. For example, with RL frameworks being deployed at scale as well as performing autonomously, it becomes imperative to incorporate explainability in them. In particular, we develop counterfactual methods to generate explanations by asking the question: <em style="color:#ffffcc">“What least change to the current policy would improve or worsen it to a new policy with a specified target return?”</em> Our generated counterfactual policies provide direct insights into how a policy can be modified to achieve better results as well as what to avoid in order not to deteriorate the performance (see figure).
                                                        </p>
                                                        <h4 id="example-related-publications">Relevant Papers:</h4>
                                                            <p><a href="https://openreview.net/pdf?id=sXYJpfoW1V" target="_blank">Towards Fair Knowledge Distillation using Student Feedback</a><br>
                                                            <a href="https://arxiv.org/pdf/2403.03744" target="_blank"> Towards Safe Large Language Models for Medicine</a><br>
                                                            <a href="https://arxiv.org/pdf/2309.16452" target="_blank">On the Trade-offs between Adversarial Robustness and Actionable Explanations</a><br>
                                                            <a href="https://proceedings.mlr.press/v139/agarwal21c/agarwal21c.pdf" target="_blank">Towards the Unification and Robustness of Perturbation and Gradient Based Explanations</a><br>
                                                            <a href="https://proceedings.mlr.press/v151/pawelczyk22a/pawelczyk22a.pdf" target="_blank">Exploring Counterfactual Explanations Through the Lens of Adversarial Examples: A Theoretical and Empirical Analysis</a><br>
                                                            <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Seth_DeAR_Debiasing_Vision-Language_Models_With_Additive_Residuals_CVPR_2023_paper.pdf" target="_blank">DeAR: Debiasing Vision-Language Models with Additive Residuals</a><br>
                                                            <a href="https://arxiv.org/pdf/2307.13192" target="_blank">Counterfactual Explanation Policies in RL</a><br>
                                                            <a href="https://arxiv.org/pdf/2309.02705" target="_blank">Certifying LLM Safety against Adversarial Prompting</a>
                                                        </p>
                                                        <p class="read-more"><a href="#" class="btn btn-danger">
                                                            <svg class="arrows">
                                                              <path class="a2" d="M0 20 L20 42 L40 20"></path>
                                                              <path class="a3" d="M0 40 L20 62 L40 40"></path>
                                                            </svg>
                                                            Read More</a>
                                                        </p>
                                                    </div> -->
                                                    <div class="line-break"><hr></div>
<!--                                                <img class="research-main-img" style="max-width: 80%; max-height: 50%;margin-bottom: 30px" src="./images/overview_research.jpg">
                                                        <div class="line-break"><hr></div> -->
                                                    </div>
                                                </article>
                                            </section>
                                        </div>
                                    </div>
                                </main>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <!-- <script type="text/javascript">
                var $el, $ps, $up, totalHeight;
                $(".research_content .btn").on("click", function() {
                  totalHeight = 0
                  $el = $(this);
                  $p  = $el.parent();
                  $up = $p.parent();
                  $ps = $up.find("p:not('.read-more')");
                  $ps.each(function() {
                    totalHeight += $(this).outerHeight();
                  });
                  $up
                    .css({
                    "height": 'fit-content',
                    "max-height": 9999
                  })
                    .animate({
                    "height": 'fit-content'
                  });
                  $p.fadeOut();
                  return false;
                });
            </script> -->
            <script type="text/javascript" src="js/menu.js" id="avia-module-menu-js"></script>
            <script type="text/javascript" src="js/menu-toggle.js" id="avia-module-toggles-js"></script>
            <script type="text/javascript" src="js/hamburger-menu.js" id="avia-hamburger-menu-js"></script>

            <script type="text/javascript" src="js/mega-menu.js" id="avia-megamenu-js"></script>
            <script type="text/javascript" src="js/sticky-header.js" id="avia-sticky-header-js"></script>
        </body>
    </html>

